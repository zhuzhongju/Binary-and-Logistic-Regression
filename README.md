# Binary-Classification-and-Logistic-Regression

<!-- TABLE OF CONTENTS -->
<details open="open">
  <summary>Table of Contents</summary>
  <ol>
    <li><a href="#binary-classification">Binary-Classification</a></li>
    <li><a href="#logistic-regression">Logistic-Regression</a></li>
  </ol>
</details>

# Binary-Classification
In a binary classification problem, the result is a discrete value output.The goal is to train a classifier that the input is represented by a feature vector, ğ‘¥, and predicts whether the corresponding label ğ‘¦ is 1 or 0.

#### Example: Cat vs Non-Cat
------------------------------------------------------------
In this case, An image is store in the computer in three separate matrices corresponding to the Red, Green, and Blue color channels of the image. The three matrices have the same size as the image, for example, the resolution of the cat image is 64 pixels X 64 pixels, the three matrices (RGB) are 64 X 64 each.
![image](https://user-images.githubusercontent.com/77977624/171976086-f14f55f2-a5bf-4212-afec-046b640a572b.png)
The value in a cell represents the pixel intensity which will be used to create a feature vector of n-dimension. In pattern recognition and machine learning, a feature vector represents an object, in this case, a cat or no cat.The pixel intensity values will be â€œunrollâ€ or â€œreshapeâ€ for each color in order to create a feature vector, ğ‘¥. 
![image](https://user-images.githubusercontent.com/77977624/171976604-fedf236a-b4d9-4537-8416-bdf92b121a24.png)

# Logistic-Regression
Logistic regression is a learning algorithm used in a supervised learning problem when the output ğ‘¦ are all either zero or one. The goal of logistic regression is to minimize the error between its predictions and training data.
#### Loss(error) Function
-----------------------------------------
The loss function measures the discrepancy between the prediction (ğ‘¦Ì‚(ğ‘–)) and the desired output (ğ‘¦(ğ‘–)).In other words, the loss function computes the error for a single training example. Generally, there are two formulas to compute the loss:

ğ¿(ğ‘¦Ì‚(ğ‘–), ğ‘¦(ğ‘–)) = 1/2(ğ‘¦Ì‚(ğ‘–) âˆ’ ğ‘¦(ğ‘–))^2 ----------------------------------(1)

ğ¿(ğ‘¦Ì‚(ğ‘–), ğ‘¦(ğ‘–)) = âˆ’( ğ‘¦(ğ‘–)log(ğ‘¦Ì‚(ğ‘–)) + (1 âˆ’ ğ‘¦(ğ‘–))log(1 âˆ’ ğ‘¦Ì‚(ğ‘–)) ----------- (2)

In practice, we are more likely to use the second formula rather than the first one as the first one may have local optimums, which would affact the outcomes of prediction. 
#### Cost Funtion
----------------------------------------------------
The cost function is the average of the loss function of the entire training set. We are going to find the parameters ğ‘¤ ğ‘ğ‘›ğ‘‘ ğ‘ that minimize the overall cost function.

![image](https://user-images.githubusercontent.com/77977624/171987201-33ba35eb-bf12-47b5-8810-65708241257f.png)
#### Example:Cat vs Non-Cat
-------------------------------------------
Given an image represented by a feature vector ğ‘¥, the algorithm will evaluate the probability of a cat being in that image.

                               ğºğ‘–ğ‘£ğ‘’ğ‘› ğ‘¥ , ğ‘¦Ì‚ = ğ‘ƒ(ğ‘¦ = 1|ğ‘¥), where 0 â‰¤ ğ‘¦Ì‚ â‰¤ 1

The parameters used in Logistic regression are:

â€¢ The input features vector: ğ‘¥ âˆˆ â„ğ‘›ğ‘¥, where ğ‘›ğ‘¥ is the number of features

â€¢ The training label: ğ‘¦ âˆˆ 0,1

â€¢ The weights: ğ‘¤ âˆˆ â„ğ‘›ğ‘¥, where ğ‘›ğ‘¥ is the number of features

â€¢ The threshold: ğ‘ âˆˆ â„

â€¢ The output: ğ‘¦Ì‚ = ğœ(ğ‘¤ğ‘‡ğ‘¥ + ğ‘)

â€¢ Sigmoid function: s = ğœ(ğ‘¤ğ‘‡ğ‘¥ + ğ‘) = ğœ(ğ‘§)= 1/1+ ğ‘’^-z
![image](https://user-images.githubusercontent.com/77977624/171987400-48e99873-a058-4b80-803e-b2eb6ff6ea38.png)
(ğ‘¤ğ‘‡ğ‘¥ + ğ‘) is a linear function (ğ‘ğ‘¥ + ğ‘), but since we are looking for a probability constraint between [0,1], the sigmoid function is used. The function is bounded between [0,1] as shown in the graph above.
